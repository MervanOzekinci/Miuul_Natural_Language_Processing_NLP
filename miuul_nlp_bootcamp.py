# -*- coding: utf-8 -*-
"""Miuul_NLP_Bootcamp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-dxTwMRshK_K64kgcm1Kup-aHgDydfGL

#1. Text Preprocessing
"""

from warnings import filterwarnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from PIL import Image
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate
from sklearn.preprocessing import LabelEncoder
from textblob import Word, TextBlob
from wordcloud import WordCloud

filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.float_format', lambda x: '%.2f' % x)

df=pd.read_csv("/content/amazon_reviews.csv",sep=",")
df.head()

df["reviewText"]

df["reviewText"]=df["reviewText"].str.lower()#bütün texti kücük harflere dönüstürelim>>>Normalization
df["reviewText"]

"""#Noktalama Isaretleri (Punctuations)"""

#burada noktalama isaretlerini silecegiz

#burada noktalama isaretlerini gördügü yerde bunu bosluk ile degistirmesini söylecegiz
df["reviewText"]=df["reviewText"].str.replace("[^\w\s]","")
df["reviewText"]

#bu noktalama isaretleri veya büyük/kücük harf anlam itibariyle birsey ifade etmedigi icin bunlar bir ölcüm problemi olarak geliyor
#bundan dolayi bu dönsüm yapildi

"""#Numbers"""

#burada sayilardan kurtulmaya calisacagiz.

df["reviewText"]=df["reviewText"].str.replace("\d","")#\d isareti sayilari ifade eder bunu bosluk ile degistirdik
df["reviewText"]

"""#Stop Words"""

#tek basina herhangi bir anlami olmayan the/for /of gibi eklemelerin veriden cikarilmasi

import nltk

nltk.download("stopwords")

from nltk.stem.snowball import stopwords
stop_Words=stopwords.words("english")

stop_Words

#yukaridaki islemi kod ile yapma
df["reviewText"]=df["reviewText"].apply(lambda x:" ".join(x for x in str(x).split() if x not in stop_Words))
df["reviewText"]

"""#Rare Words(Nadir gecen kelimeler)"""

temp_df=pd.Series(" ".join(df["reviewText"]).split()).value_counts()#hangi kelimeden kactane gectigini burda yazdiriyoruz
temp_df

drops=temp_df[temp_df<=1]
drops

df["reviewText"]=df["reviewText"].apply(lambda x:" ".join(x for x in str(x).split() if x not in drops))#bu yukarida yapilanlari burdan atalim
df["reviewText"]

"""#Tokenization"""

#cumleleri parcalara ayirmak.Yani birimlestirmek.

nltk.download("punkt")

df["reviewText"].apply(lambda x:TextBlob(x).words).head()#cümleleri burada parcalara ayirdik

"""#Lemmatization >Kelimeleri köklerine ayirmak>Gözlük>Gözlükcü>Göz"""

nltk.download("wordnet")
nltk.download('omw-1.4')

df["reviewText"]=df["reviewText"].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
df["reviewText"]

"""#Metin Görsellestirme"""

#metinlerdeki kelimelerin frekanslarini olusturduktan sonra bunlari istenilen sekilde görsellestirebiliriz.

tf=df["reviewText"].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis=0).reset_index()
tf#bütün kelimelerin essiz olanlarin sayisi cikarildi

tf.columns=["words","tf"]

tf.sort_values("tf", ascending=False)  #azalan sekilde siralmasi yapildi

"""#Sutun Grafik(Bar Plot) Olusturulmasi

"""

tf[tf["tf"]>500]

tf[tf["tf"]>500].plot.bar(x="words",y="tf")
plt.show()

"""#Kelime Bulutu (Word Cloud)"""

text=" ".join(i for i in df.reviewText)
text

wordcloud=WordCloud().generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

wordcloud=WordCloud(max_font_size=50,
                    max_words=100,
                    background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""#Sablonlara Göre Word Cloud"""

solution_mask=np.array(Image.open("/content/solution.png"))

wc=WordCloud(mask=solution_mask,
                    contour_width=3,
                    contour_color="firebrick",
                    max_words=1000,
                    background_color="white").generate(text)
plt.figure(figsize=[10,10])
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

"""# Duygu Durum Analizi(Sentiment Analysis)"""

df["reviewText"].head()

nltk.download("vader_lexicon")

sia=SentimentIntensityAnalyzer()

sia.polarity_scores("The film was awesome")
#compound sifirdan büyükse pozitig

sia.polarity_scores("I liked this music but it is not good as the other one")

df["reviewText"][0:10].apply(lambda x:sia.polarity_scores(x))

df["reviewText"][0:10].apply(lambda x:sia.polarity_scores(x)["compound"])

df["polarity_score"]=df["reviewText"].apply(lambda x:sia.polarity_scores(x)["compound"])
df

"""# Duygu Durum Modelleme(Sentiment Modeling)

##Fetaure Enineering
"""

df["reviewText"][0:10].apply(lambda x:"pos" if sia.polarity_scores(x)["compound"]>0 else "neg")

df["sentiment_label"]=df["reviewText"].apply(lambda x:"pos" if sia.polarity_scores(x)["compound"]>0 else "neg")

df["sentiment_label"].value_counts()

df.groupby("sentiment_label")["overall"].mean()

df["sentiment_label"]=LabelEncoder().fit_transform(df["sentiment_label"])
df["sentiment_label"]

y=df["sentiment_label"]
X=df["reviewText"]

"""#Countvectors >kelimeleri numeric halleriyle ele almak"""

# Count Vectors_Frekans temsiller
#TF-IDF Vectors:normalize edilmis frekans temsiller
#Word Embeddings(Word2Vec,Glove,BERT vs)

#countvectors>kelimelerin kacar defa gectigini cikarmak

#words
##Kelimelerin nümerik temsilleri

#characters
##Karakterlerin numerik temsilleri

#ngram
a="""Bu örnegi anlasilabilmesi icin daha uzun bir metin üzerinden gösterecegim. N_graml'öar birlikte kullanilan kelimelerin
kombinasyonlarini gösterir ve feature üretmek icin kullanilir """

TextBlob(a).ngrams(3)#3lü kombinasyonlar olusturmak

#count vektors
from sklearn.feature_extraction.text import CountVectorizer

corpus=["This is the first document.","This document is the second document.",
        "And this is the third one.","Is this the first document?"]
corpus

#word frekans
vectorizer=CountVectorizer()
X_c=vectorizer.fit_transform(corpus)
vectorizer.get_feature_names()
X_c.toarray()

vectorizer.get_feature_names()

#n-gram frekans
vectorizer2=CountVectorizer(analyzer="word", ngram_range=(2,2))
X_n=vectorizer2.fit_transform(corpus)
vectorizer2.get_feature_names()
X_n.toarray()

vectorizer2.get_feature_names()

#simdi bunu kendi verimiz üzerinde deneyelim
vectorizer=CountVectorizer()
X_count=vectorizer.fit_transform(X)

vectorizer.get_feature_names()[10:15]

X_count.toarray()[10:15]

"""#TF-IDF Yöntemi"""

#CountVector yönteminin ortaya cikarabilecegi bazi yanliliklari giderebilmek adina normalize edilmis kelime olusturma vektorüdür.

from sklearn.feature_extraction.text import TfidfVectorizer
tf_idf_word_vectorizer=TfidfVectorizer()

X_tf_idf_word=tf_idf_word_vectorizer.fit_transform(X)

tf_idf_ngram_vectorizer=TfidfVectorizer(ngram_range=(2,3))
X_tf_idf_ngram=tf_idf_ngram_vectorizer.fit_transform(X)

"""#Sentiment Modeling"""

# 1.Text Preprocessing
# 2.Text Visualization
# 3.Sentiment Analysis
# 4.Feature Engineering
# 5.Sentiment Modeling<<<

##########################
#Logistic Regression
##########################

log_model=LogisticRegression().fit(X_tf_idf_word,y)

cross_val_score(log_model,
                X_tf_idf_word,
                y,scoring="accuracy",
                cv=5).mean()

new_review=pd.Series("this product is look good")#yeni bir yorum olsun ve bunun modelimizdeki degerlendirmesine bakalim

#yeni yorum suan string boyutunda bunu simdi vectöre dönüstürüp böyle sormamiz lazim modelimize
new_review=TfidfVectorizer().fit(X).transform(new_review)

new_review2=pd.Series("look at that shit very bad")#yeni bir yorum olsun ve bunun modelimizdeki degerlendirmesine bakalim
#yeni yorum suan string boyutunda bunu simdi vectöre dönüstürüp böyle sormamiz lazim modelimize
new_review2=TfidfVectorizer().fit(X).transform(new_review2)

log_model.predict(new_review)

log_model.predict(new_review2)

random_review=pd.Series(df["reviewText"].sample(1).values)
random_review

new_reveiw=TfidfVectorizer().fit(X).transform(random_review)

log_model.predict(new_review)

"""#Random Forests"""

#Count Vectors
rf_model=RandomForestClassifier().fit(X_count,y)
cross_val_score(rf_model,X_count,y,cv=5,n_jobs=-1).mean()#n_jobs=bütün islemcileri kullan

#TF-IDF Word-Level
rf_model=RandomForestClassifier().fit(X_tf_idf_word,y)
cross_val_score(rf_model,X_tf_idf_word,y,cv=5,n_jobs=-1).mean()

# TF-IDF N-GRAM
rf_model=RandomForestClassifier().fit(X_tf_idf_ngram,y)
cross_val_score(rf_model,X_tf_idf_ngram,y,cv=5,n_jobs=-1).mean()

"""#Hiperparametre Optimizasyonu"""

rf_model=RandomForestClassifier(random_state=15)

rf_params={"max_depth":[8,None],
           "max_features":[7,"auto"],
           "min_samples_split":[2,5,8],
           "n_estimators":[100,200]}
#burada kurulacak model icin en iyi parametrelerin ayarlanmasi

rf_best_grid=GridSearchCV(rf_model,
                          rf_params,
                          cv=5,
                          n_jobs=-1,
                          verbose=1).fit(X_count,y)#verbose=raporlama yapilsin mi
#modelime en uygun paramtrelerin hangisi oldugun söyle

rf_best_grid.best_params_#en iyi parametreler

rf_final=rf_model.set_params(**rf_best_grid.best_params_, random_state=15).fit(X_count,y)

cross_val_score(rf_final,X_count,y,cv=5,n_jobs=-1).mean()

##simdi biraz daha parametre optimizasyonu ile daha iyi bir sonuc alabilir miyiz ona bakalim.

rf_params={"max_depth":[5,8,15,20,None],
           "max_features":[5,7,15,20,"auto"],
           "min_samples_split":[2,5,8,12],
           "n_estimators":[100,200,300,500]}
#burada kurulacak model icin en iyi parametrelerin ayarlanmasi
rf_best_grid=GridSearchCV(rf_model,
                          rf_params,
                          cv=5,
                          n_jobs=-1,
                          verbose=1).fit(X_count,y)#verbose=raporlama yapilsin mi
#modelime en uygun paramtrelerin hangisi oldugun söyle

rf_best_grid.best_params_#en iyi parametreler

rf_final=rf_model.set_params(**rf_best_grid.best_params_, random_state=15).fit(X_count,y)

cross_val_score(rf_final,X_count,y,cv=5,n_jobs=-1).mean()